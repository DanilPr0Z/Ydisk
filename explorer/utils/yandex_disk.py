
import requests
from django.conf import settings
from django.core.cache import cache
import urllib.parse
import concurrent.futures
import time
import threading
import re
from urllib.parse import urlparse
import queue
import asyncio
import aiohttp


class YandexDiskClient:
    def __init__(self):
        self.api_base_url = settings.YANDEX_DISK_CONFIG['API_BASE_URL']
        self.oauth_token = settings.YANDEX_DISK_CONFIG['OAUTH_TOKEN']
        self.root_folder = settings.YANDEX_DISK_CONFIG['ROOT_FOLDER']
        self.max_workers = getattr(settings, 'YANDEX_MAX_WORKERS', 16)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–æ—Ä–∫–µ—Ä—ã
        self.request_timeout = getattr(settings, 'REQUEST_TIMEOUT', 30)
        self.headers = {
            'Authorization': f'OAuth {self.oauth_token}',
            'Accept': 'application/json'
        }
        self._rate_limit_semaphore = threading.Semaphore(20)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
        self._last_request_time = 0
        self._min_request_interval = 0.02  # –£–º–µ–Ω—å—à–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª
        self._share_cache = {}
        self._download_cache = {}
        self._cache_lock = threading.Lock()

    def _make_request(self, url, params=None, method='GET'):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å rate limiting"""
        with self._rate_limit_semaphore:
            current_time = time.time()
            time_since_last_request = current_time - self._last_request_time
            if time_since_last_request < self._min_request_interval:
                time.sleep(self._min_request_interval - time_since_last_request)

            self._last_request_time = time.time()

            try:
                if method == 'GET':
                    response = requests.get(url, headers=self.headers, params=params, timeout=self.request_timeout)
                elif method == 'PUT':
                    response = requests.put(url, headers=self.headers, params=params, timeout=self.request_timeout)

                if response.status_code == 404:
                    return None
                elif response.status_code == 429:
                    print("‚ö†Ô∏è Rate limit hit, implementing backoff...")
                    time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –ø—Ä–∏ rate limit
                    return None
                elif response.status_code != 200:
                    return None

                return response.json()
            except requests.exceptions.Timeout:
                print("‚è∞ Request timeout")
                return None
            except requests.exceptions.RequestException as e:
                print(f"‚ùå API Request error: {e}")
                return None

    def get_folder_contents(self, path=''):
        """–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏"""
        if not path:
            path = self.root_folder

        if not path.startswith('disk:/'):
            full_path = f"disk:/{path}"
        else:
            full_path = path

        cache_key = f"folder_{hash(full_path)}"
        cached_data = cache.get(cache_key)

        if cached_data:
            return cached_data

        print(f"üîç Fetching contents for path: '{full_path}'")

        url = f"{self.api_base_url}"
        params = {
            'path': full_path,
            'limit': 1000
        }

        data = self._make_request(url, params)

        if data and '_embedded' in data and 'items' in data['_embedded']:
            items = data['_embedded']['items']
            print(f"‚úÖ Found {len(items)} items in '{full_path}'")
            cache.set(cache_key, items, timeout=7200)
            return items

        return []

    def get_flat_file_list(self):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
        cache_key = "all_files_optimized_v5"
        cached_data = cache.get(cache_key)

        if cached_data:
            print("‚úÖ Using optimized file cache")
            return cached_data

        print(f"üöÄ HIGH-PERFORMANCE: Building file list with {self.max_workers} parallel workers...")
        start_time = time.time()

        all_files = []
        folders_to_process = [self.root_folder]
        processed_folders = set()
        folder_lock = threading.Lock()

        def process_folder_batch(folder_batch):
            """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø–∞–ø–æ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
            batch_files = []
            new_folders = []

            for folder_path in folder_batch:
                if folder_path in processed_folders:
                    continue

                with folder_lock:
                    processed_folders.add(folder_path)

                items = self.get_folder_contents(folder_path)
                if not items:
                    continue

                for item in items:
                    if item['type'] == 'file':
                        batch_files.append({
                            'name': item['name'],
                            'path': item['path'],
                            'size': item.get('size', 0),
                            'modified': item.get('modified', ''),
                            'media_type': item.get('media_type', 'file'),
                            'name_lower': item['name'].lower()
                        })
                    elif item['type'] == 'dir':
                        new_folders.append(item['path'])

            return batch_files, new_folders

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            while folders_to_process:
                batch_size = min(len(folders_to_process), self.max_workers * 5)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞—Ç—á
                current_batch = folders_to_process[:batch_size]
                folders_to_process = folders_to_process[batch_size:]

                future_to_batch = {
                    executor.submit(process_folder_batch, [folder]): folder
                    for folder in current_batch
                }

                for future in concurrent.futures.as_completed(future_to_batch):
                    try:
                        batch_files, new_folders = future.result()
                        all_files.extend(batch_files)
                        folders_to_process.extend(new_folders)
                    except Exception as e:
                        print(f"‚ùå Error processing folder batch: {e}")

        total_time = time.time() - start_time
        print(f"‚úÖ HIGH-PERFORMANCE: Built file list with {len(all_files)} files in {total_time:.2f}s "
              f"({len(all_files) / total_time:.1f} files/sec)")

        cache.set(cache_key, all_files, timeout=7200)
        return all_files

    def get_file_download_link(self, path):
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
        with self._cache_lock:
            if path in self._download_cache:
                return self._download_cache[path]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ Django cache
        cache_key = f"download_{hash(path)}"
        cached_link = cache.get(cache_key)

        if cached_link:
            with self._cache_lock:
                self._download_cache[path] = cached_link
            return cached_link

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—É—é —Å—Å—ã–ª–∫—É
        url = f"{self.api_base_url}/download"
        params = {'path': path}

        data = self._make_request(url, params)
        if data and 'href' in data:
            download_link = data['href']
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à–∏
            cache.set(cache_key, download_link, timeout=7200)
            with self._cache_lock:
                self._download_cache[path] = download_link
            return download_link

        return None

    def get_public_share_link(self, path):
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—É–±–ª–∏—á–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
        with self._cache_lock:
            if path in self._share_cache:
                return self._share_cache[path]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ Django cache
        cache_key = f"public_{hash(path)}"
        cached_link = cache.get(cache_key)

        if cached_link:
            with self._cache_lock:
                self._share_cache[path] = cached_link
            return cached_link

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—É—é —Å—Å—ã–ª–∫—É
        public_link = self._get_fresh_public_link(path)

        if public_link:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à–∏
            cache.set(cache_key, public_link, timeout=86400)
            with self._cache_lock:
                self._share_cache[path] = public_link

        return public_link

    def _get_fresh_public_link(self, path):
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤—É—é –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # –ü—É–±–ª–∏–∫—É–µ–º —Ñ–∞–π–ª
                publish_url = f"{self.api_base_url}/publish"
                publish_params = {'path': path}

                publish_data = self._make_request(publish_url, publish_params, method='PUT')
                if not publish_data:
                    if attempt < max_retries - 1:
                        time.sleep(0.5 * (attempt + 1))  # Exponential backoff
                        continue
                    return None

                # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —Å—Å—ã–ª–∫–∏
                time.sleep(0.1)

                # –ü–æ–ª—É—á–∞–µ–º –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É
                share_url = f"{self.api_base_url}"
                share_params = {
                    'path': path,
                    'fields': 'public_url'
                }

                share_data = self._make_request(share_url, share_params)
                if share_data and 'public_url' in share_data:
                    return share_data['public_url']
                else:
                    if attempt < max_retries - 1:
                        time.sleep(0.5 * (attempt + 1))
                        continue

            except Exception as e:
                print(f"‚ùå Error getting public link for {path}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))
                    continue

        return None

    def _process_single_file_links(self, file_path):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        path = file_path['path']
        try:
            download_link = self.get_file_download_link(path)
            public_link = self.get_public_share_link(path)

            return {
                'path': path,
                'download_link': download_link,
                'public_link': public_link,
                'success': True
            }
        except Exception as e:
            print(f"‚ùå Error processing links for {path}: {e}")
            return {
                'path': path,
                'download_link': None,
                'public_link': None,
                'success': False,
                'error': str(e)
            }

    def batch_get_links_hyper_optimized(self, file_paths):
        """–ì–ò–ü–ï–†-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫"""
        print(f"üöÄ HYPER-OPTIMIZED: Processing {len(file_paths)} files with {self.max_workers} threads...")
        start_time = time.time()

        results = []
        total_files = len(file_paths)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # –°–æ–∑–¥–∞–µ–º futures –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            future_to_path = {
                executor.submit(self._process_single_file_links, fp): fp['path']
                for fp in file_paths
            }

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            completed = 0
            for future in concurrent.futures.as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1

                    # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
                    if completed % 50 == 0:
                        elapsed = time.time() - start_time
                        speed = completed / elapsed if elapsed > 0 else 0
                        print(f"üìä Progress: {completed}/{total_files} "
                              f"({completed / total_files * 100:.1f}%) - "
                              f"{speed:.1f} files/sec")

                except Exception as e:
                    print(f"‚ùå Unexpected error for {path}: {e}")
                    results.append({
                        'path': path,
                        'download_link': None,
                        'public_link': None,
                        'success': False,
                        'error': str(e)
                    })
                    completed += 1

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        successful = sum(1 for r in results if r.get('success', False))
        total_time = time.time() - start_time

        print(f"‚úÖ HYPER-OPTIMIZED: Completed {len(results)} files in {total_time:.2f}s "
              f"({len(results) / total_time:.1f} files/sec) - "
              f"Success: {successful}/{len(results)} ({successful / len(results) * 100:.1f}%)")

        return results

    def mass_preload_all_links(self, all_files, batch_size=100):
        """–ú–ê–°–°–û–í–ê–Ø –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º"""
        print(f"üöÄ MASS PRELOAD: Starting mass links preloading for {len(all_files)} files...")
        start_time = time.time()

        total_files = len(all_files)
        total_processed = 0
        total_successful = 0

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –±–∞—Ç—á–∞–º–∏
        for i in range(0, total_files, batch_size):
            batch_files = all_files[i:i + batch_size]
            file_paths = [{'path': file['path']} for file in batch_files]

            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –±–∞—Ç—á–∞
            batch_results = self.batch_get_links_hyper_optimized(file_paths)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∞
            batch_successful = sum(1 for r in batch_results if r.get('success', False))
            total_successful += batch_successful
            total_processed += len(batch_results)

            progress = min(i + batch_size, total_files)
            elapsed = time.time() - start_time
            overall_speed = total_processed / elapsed if elapsed > 0 else 0

            print(f"üìà BATCH {i // batch_size + 1}: {batch_successful}/{len(batch_results)} successful | "
                  f"Overall: {total_successful}/{total_processed} | "
                  f"Speed: {overall_speed:.1f} files/sec")

            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + batch_size < total_files:
                pause = max(0.5, 2.0 - (overall_speed / 10))  # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞—É–∑–∞
                time.sleep(pause)

        total_time = time.time() - start_time
        success_rate = (total_successful / total_files) * 100

        print(f"üéâ MASS PRELOAD COMPLETED: {total_files} files in {total_time:.2f}s "
              f"({total_files / total_time:.1f} files/sec)")
        print(f"üìä SUCCESS RATE: {total_successful}/{total_files} ({success_rate:.1f}%)")

        return total_successful

    def get_relative_path(self, full_path):
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏"""
        if full_path.startswith('disk:/'):
            full_path = full_path[6:]

        if full_path.startswith(self.root_folder):
            relative = full_path[len(self.root_folder):].lstrip('/')
            return relative
        return full_path

    def build_search_index(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        cache_key = "search_index"
        cached_index = cache.get(cache_key)

        if cached_index:
            return cached_index

        print("üîç Building search index...")
        all_files = self.get_flat_file_list()

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å: —Å–ª–æ–≤–æ -> —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        search_index = {}
        for file_item in all_files:
            file_name_lower = file_item['name'].lower()
            words = re.findall(r'\b\w+\b', file_name_lower)

            for word in words:
                if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    if word not in search_index:
                        search_index[word] = []
                    search_index[word].append(file_item)

        cache.set(cache_key, search_index, timeout=3600)
        print(f"‚úÖ Search index built: {len(search_index)} words")
        return search_index

    def get_folder_public_link(self, path):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É –¥–ª—è –ø–∞–ø–∫–∏"""
        cache_key = f"folder_public_{hash(path)}"
        cached_link = cache.get(cache_key)

        if cached_link:
            return cached_link

        # –î–ª—è –ø–∞–ø–æ–∫ —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É
        publish_url = f"{self.api_base_url}/publish"
        publish_params = {'path': path}

        publish_data = self._make_request(publish_url, publish_params, method='PUT')
        if not publish_data:
            return None

        time.sleep(0.3)

        share_url = f"{self.api_base_url}"
        share_params = {
            'path': path,
            'fields': 'public_url'
        }

        share_data = self._make_request(share_url, share_params)
        if share_data and 'public_url' in share_data:
            public_url = share_data['public_url']
            cache.set(cache_key, public_url, timeout=86400)
            return public_url

        return None
